{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import ceil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "#torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "dataset_path = \"../Dataset\"\n",
    "models_path  = \"../models\"\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  _DEVICE = \"cuda\"\n",
    "else:\n",
    "  _DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def count_parameters(NN):\n",
    "    return sum([p.numel() for p in NN.parameters() if p.requires_grad==True])\n",
    "\n",
    "def get_one_hot_encoder(labels):\n",
    "    def ohe(target):\n",
    "        oh_vec=torch.zeros(len(labels), dtype=torch.float)\n",
    "        oh_vec[target] = 1.\n",
    "        return oh_vec\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "dataset = datasets.ImageFolder(f'{dataset_path}/Train')\n",
    "dataset = datasets.ImageFolder(f'{dataset_path}/Train', transform=transform, target_transform=get_one_hot_encoder(dataset.classes))\n",
    "\n",
    "dataset_train, dataset_val = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), \n",
    "                                                                     ceil(len(dataset)*0.2)])\n",
    "\n",
    "dataset_test  = datasets.ImageFolder(f'{dataset_path}/Test', transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = data.DataLoader(dataset_test,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = data.DataLoader(dataset_val,   batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
    "valSteps   = len(val_loader.dataset)  // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/nzuri/HNS/HNS_Znacky_Zadanie1/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nzuri/HNS/HNS_Znacky_Zadanie1/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained AlexNet\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Feezing parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=9216, out_features=256, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=256, out_features=256, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(in_features=256, out_features=len(dataset.classes), bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters: 2428428\n"
     ]
    }
   ],
   "source": [
    "print(f\"trainable parameters: {count_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 100\n",
    "\n",
    "model.to(_DEVICE)\n",
    "progress_bar = tqdm(range(0, N_EPOCH))\n",
    "H = {\"train_loss\": [],\t  \"val_loss\": []}\n",
    "for epoch in progress_bar:\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(_DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.to(torch.float32).to(_DEVICE)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      for inputs, targets in val_loader:\n",
    "        inputs = inputs.to(_DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        targets = targets.to(_DEVICE)\n",
    "        totalValLoss += loss_fn(outputs, targets)\n",
    "\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss   = totalValLoss / valSteps\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    progress_bar.set_description(f\"Train loss: {avgTrainLoss:.6f} | Val loss: {avgValLoss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{models_path}/v1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 119/128 [03:01<00:06,  1.45it/s]"
     ]
    }
   ],
   "source": [
    "predictions = list()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs = inputs.to(_DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(\n",
    "            [inputs.cpu().detach().numpy(), outputs.cpu().detach().numpy()]\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
